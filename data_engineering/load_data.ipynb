{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Spark Setup\n",
    "Create a spark session and the UDFs required to parse the data into the income/population proxies\n",
    "\n",
    "**Libraries Required**\n",
    "- [PySpark](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [uszipcode](https://pypi.org/project/uszipcode/)\n",
    "\n",
    "You have to extract the simple_db.sqlite database ([available here](https://github.com/MacHu-GWU/uszipcode-project/releases/download/1.0.1.db/simple_db.sqlite)) and place in a data folder along with the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get the /data folder in the project (not tracked by git)\n",
    "datapath = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), 'data/')\n",
    "# name of the sqlite jdbc driver jar stored in /data\n",
    "dbjar = 'sqlite-jdbc-3.39.4.0.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# change to spark.jars to your own address\n",
    "# Create a spark session to process the data\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.jars\", os.path.join(datapath, dbjar))\\\n",
    "    .master(\"local[*]\").getOrCreate()  # running at :4040\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_db = os.path.join(datapath, 'simple_db.sqlite')\n",
    "location_df = spark.read.format(\"JDBC\")\\\n",
    "    .option(\"url\", f\"jdbc:sqlite:{sqlite_db}\")\\\n",
    "    .option(\"dbtable\", \"simple_zipcode\")\\\n",
    "    .option(\"driver\", \"org.sqlite.JDBC\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df.createOrReplaceTempView(\"zip_data_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(address)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('zip_converter', lambda address: address.split()[-1]) # splits the address string in realtor_data and selects the zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Read data from the rental housing dataset\n",
    "\n",
    "Source: [Kaggle(austinreese)](https://www.kaggle.com/datasets/austinreese/usa-housing-listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../data/housing.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- region_url: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- sqfeet: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- baths: string (nullable = true)\n",
      " |-- cats_allowed: string (nullable = true)\n",
      " |-- dogs_allowed: string (nullable = true)\n",
      " |-- smoking_allowed: string (nullable = true)\n",
      " |-- wheelchair_access: string (nullable = true)\n",
      " |-- electric_vehicle_charge: string (nullable = true)\n",
      " |-- comes_furnished: string (nullable = true)\n",
      " |-- laundry_options: string (nullable = true)\n",
      " |-- parking_options: string (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"rentals\") # load the dataframe as a queryable table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rent_df = spark.sql(\"\"\"\n",
    "WITH rental_data AS (\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY (SELECT 1)) AS number,\n",
    "        DOUBLE(sqfeet) AS sqfeet,\n",
    "        DOUBLE(beds) AS beds, \n",
    "        DOUBLE(baths) AS baths,\n",
    "        DOUBLE(price) AS price,\n",
    "        DOUBLE(lat) AS lat, \n",
    "        DOUBLE(long) AS long\n",
    "    FROM rentals\n",
    "    WHERE \n",
    "        sqfeet IS NOT NULL AND \n",
    "        beds IS NOT NULL AND \n",
    "        baths IS NOT NULL AND \n",
    "        price IS NOT NULL AND\n",
    "        lat IS NOT NULL AND\n",
    "        long IS NOT NULL \n",
    ")\n",
    "SELECT\n",
    "    sqfeet,\n",
    "    beds,\n",
    "    baths,\n",
    "    price,\n",
    "    population_density AS density,\n",
    "    median_household_income AS income,\n",
    "    post_office_city AS label\n",
    "FROM(\n",
    "    SELECT \n",
    "        A.number, \n",
    "        A.sqfeet, \n",
    "        A.beds, \n",
    "        A.baths, \n",
    "        A.price, \n",
    "        A.lat, \n",
    "        A.long,\n",
    "        B.population_density,\n",
    "        B.median_household_income,\n",
    "        B.post_office_city,\n",
    "        B.state,\n",
    "        RANK () OVER ( \n",
    "            PARTITION BY A.number\n",
    "            ORDER BY \n",
    "                SQRT(\n",
    "                    POWER((B.bounds_north + B.bounds_south)/2-A.lat, 2) + \n",
    "                    POWER((B.bounds_west + B.bounds_east)/2-A.long, 2)\n",
    "                )\n",
    "            ASC\n",
    "        ) AS rank \n",
    "    FROM \n",
    "        rental_data AS A,\n",
    "        zip_data_tbl AS B\n",
    "    WHERE \n",
    "        A.lat BETWEEN B.bounds_south AND B.bounds_north\n",
    "        AND A.long BETWEEN B.bounds_west AND B.bounds_east\n",
    "        AND B.population_density IS NOT NULL\n",
    "        AND B.median_household_income IS NOT NULL\n",
    ")\n",
    "WHERE rank = 1\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sqfeet: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- baths: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- density: float (nullable = true)\n",
      " |-- income: integer (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_rent_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sqfeet: double, beds: double, baths: double, price: double, density: float, income: int, label: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rent_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------+-------+------+-------------+\n",
      "|sqfeet|beds|baths| price|density|income|        label|\n",
      "+------+----+-----+------+-------+------+-------------+\n",
      "| 739.0| 1.0|  1.0|1177.0| 3741.0| 37110|   Sparks, NV|\n",
      "| 250.0| 0.0|  1.0| 309.0| 6709.0| 22345|     Reno, NV|\n",
      "| 900.0| 2.0|  1.0|1195.0| 4058.0| 31081|     Reno, NV|\n",
      "|1400.0| 3.0|  2.5| 988.0| 1938.0| 85479|Camarillo, CA|\n",
      "| 270.0| 0.0|  1.0| 249.0| 6709.0| 22345|     Reno, NV|\n",
      "+------+----+-----+------+-------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_rent_df.show(5) # will compute the entire table (comment out if you only want computation at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368690"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rent_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sqfeet: double, beds: double, baths: double, price: double, density: float, income: int, label: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rent_df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rent_df.createOrReplaceTempView('rental_values') # save the final data as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Read data from the housing listings dataset\n",
    "\n",
    "Source: [Kaggle(ahmedshahriarsakib)](https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../data/realtor-data.csv\", header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- status: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- bed: string (nullable = true)\n",
      " |-- bath: string (nullable = true)\n",
      " |-- acre_lot: string (nullable = true)\n",
      " |-- full_address: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- house_size: string (nullable = true)\n",
      " |-- sold_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---+----+--------+--------------------+--------------------+-------------+-----------+--------+----------+---------+\n",
      "|  status|   price|bed|bath|acre_lot|        full_address|              street|         city|      state|zip_code|house_size|sold_date|\n",
      "+--------+--------+---+----+--------+--------------------+--------------------+-------------+-----------+--------+----------+---------+\n",
      "|for_sale|105000.0|3.0| 2.0|    0.12|Sector Yahuecas T...|Sector Yahuecas T...|     Adjuntas|Puerto Rico|   601.0|     920.0|     null|\n",
      "|for_sale| 80000.0|4.0| 2.0|    0.08|Km 78 9 Carr # 13...|  Km 78 9 Carr # 135|     Adjuntas|Puerto Rico|   601.0|    1527.0|     null|\n",
      "|for_sale| 67000.0|2.0| 1.0|    0.15|556G 556-G 16 St,...|    556G 556-G 16 St|   Juana Diaz|Puerto Rico|   795.0|     748.0|     null|\n",
      "|for_sale|145000.0|4.0| 2.0|     0.1|R5 Comunidad El P...|R5 Comunidad El P...|        Ponce|Puerto Rico|   731.0|    1800.0|     null|\n",
      "|for_sale| 65000.0|6.0| 2.0|    0.05|14 Navarro, Mayag...|          14 Navarro|     Mayaguez|Puerto Rico|   680.0|      null|     null|\n",
      "|for_sale|179000.0|4.0| 3.0|    0.46|Bo Calabazas San ...|Bo Calabazas San ...|San Sebastian|Puerto Rico|   612.0|    2520.0|     null|\n",
      "|for_sale| 50000.0|3.0| 1.0|     0.2|49.1 140, Ciales,...|            49.1 140|       Ciales|Puerto Rico|   639.0|    2040.0|     null|\n",
      "|for_sale| 71600.0|3.0| 2.0|    0.08|3467 St, Ponce, P...|             3467 St|        Ponce|Puerto Rico|   731.0|    1050.0|     null|\n",
      "|for_sale|100000.0|2.0| 1.0|    0.09|230 Rio De Vida, ...|     230 Rio De Vida|        Ponce|Puerto Rico|   730.0|    1092.0|     null|\n",
      "|for_sale|300000.0|5.0| 3.0|    7.46|Pr 120 Bo Maravil...|Pr 120 Bo Maravil...|   Las Marias|Puerto Rico|   670.0|    5403.0|     null|\n",
      "|for_sale| 89000.0|3.0| 2.0|   13.39|Km 3 4 Solar 457 ...|Km 3 4 Solar 457 ...|      Isabela|Puerto Rico|   662.0|    1106.0|     null|\n",
      "|for_sale|150000.0|3.0| 2.0|    0.08|91 Del Rio, Juana...|          91 Del Rio|   Juana Diaz|Puerto Rico|   795.0|    1045.0|     null|\n",
      "|for_sale|155000.0|3.0| 2.0|     0.1|Pr, Lares, PR, 00669|                  Pr|        Lares|Puerto Rico|   669.0|    4161.0|     null|\n",
      "|for_sale| 79000.0|5.0| 2.0|    0.12|90 # A10, Utuado,...|            90 # A10|       Utuado|Puerto Rico|   641.0|    1620.0|     null|\n",
      "|for_sale|649000.0|5.0| 5.0|    0.74|F118 Madrid St El...|F118 Madrid St El...|        Ponce|Puerto Rico|   731.0|    2677.0|     null|\n",
      "|for_sale|120000.0|3.0| 2.0|    0.08|10-K Alejandrina ...| 10-K Alejandrina St|        Yauco|Puerto Rico|   698.0|    1100.0|     null|\n",
      "|for_sale|235000.0|4.0| 4.0|    0.22|10 Calle Carrau #...|10 Calle Carrau #...|     Mayaguez|Puerto Rico|   680.0|    3450.0|     null|\n",
      "|for_sale|105000.0|3.0| 2.0|    0.08|DD18 Calle 28, Po...|       DD18 Calle 28|        Ponce|Puerto Rico|   728.0|    1500.0|     null|\n",
      "|for_sale|575000.0|3.0| 2.0|    3.88|5.8 Carr 435 Km #...|5.8 Carr 435 Km # 58|San Sebastian|Puerto Rico|   685.0|    4000.0|     null|\n",
      "|for_sale|140000.0|6.0| 3.0|    0.25|1 Bo Corcovada, A...|      1 Bo Corcovada|       Anasco|Puerto Rico|   610.0|    1230.0|     null|\n",
      "+--------+--------+---+----+--------+--------------------+--------------------+-------------+-----------+--------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sale_df = spark.sql(f\"\"\"\n",
    "WITH sales_data AS (\n",
    "    SELECT \n",
    "        DOUBLE(house_size) AS sqfeet,\n",
    "        DOUBLE(bed) AS beds, \n",
    "        DOUBLE(bath) AS baths,\n",
    "        DOUBLE(price),\n",
    "        zip_converter(full_address) AS zip\n",
    "    FROM sales\n",
    "    WHERE \n",
    "        house_size IS NOT NULL AND \n",
    "        bed IS NOT NULL AND \n",
    "        bath IS NOT NULL AND \n",
    "        price IS NOT NULL AND\n",
    "        full_address IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    A.sqfeet, \n",
    "    A.beds, \n",
    "    A.baths, \n",
    "    A.price, \n",
    "    A.zip,\n",
    "    B.population_density AS density,\n",
    "    B.median_household_income AS income,\n",
    "    B.post_office_city AS label\n",
    "FROM \n",
    "    sales_data AS A\n",
    "    JOIN\n",
    "    zip_data_tbl AS B\n",
    "    ON \n",
    "    A.zip = B.zipcode\n",
    "WHERE\n",
    "    B.population_density IS NOT NULL\n",
    "    AND B.median_household_income IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sqfeet: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- baths: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- density: float (nullable = true)\n",
      " |-- income: integer (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sale_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sqfeet: double, beds: double, baths: double, price: double, zip: string, density: float, income: int, label: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sale_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+---------+-----+-------+------+----------+\n",
      "|sqfeet|beds|baths|    price|  zip|density|income|     label|\n",
      "+------+----+-----+---------+-----+-------+------+----------+\n",
      "|4000.0| 4.0|  4.0|1200000.0|02053| 1105.0|106132|Medway, MA|\n",
      "|3024.0| 3.0|  3.0| 979900.0|02053| 1105.0|106132|Medway, MA|\n",
      "|1400.0| 3.0|  3.0| 539900.0|02053| 1105.0|106132|Medway, MA|\n",
      "|2284.0| 4.0|  1.0| 599900.0|02053| 1105.0|106132|Medway, MA|\n",
      "|3697.0| 4.0|  4.0| 929900.0|02053| 1105.0|106132|Medway, MA|\n",
      "+------+----+-----+---------+-----+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sale_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579006"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sale_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sqfeet: double, beds: double, baths: double, price: double, zip: string, density: float, income: int, label: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sale_df.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sale_df.createOrReplaceTempView('sale_values') # will compute the entire table (comment out if you only want computation at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Saving data into parquets to read by pandas\n",
    "We want to use parquet instead of csv because it is smaller and faster to read, but you lose human readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------+-------+------+\n",
      "|sqfeet|beds|baths|income|density| price|\n",
      "+------+----+-----+------+-------+------+\n",
      "| 739.0| 1.0|  1.0| 37110| 3741.0|1177.0|\n",
      "| 250.0| 0.0|  1.0| 22345| 6709.0| 309.0|\n",
      "| 900.0| 2.0|  1.0| 31081| 4058.0|1195.0|\n",
      "|1400.0| 3.0|  2.5| 85479| 1938.0| 988.0|\n",
      "| 270.0| 0.0|  1.0| 22345| 6709.0| 249.0|\n",
      "|1117.0| 3.0|  2.0| 39912| 4300.0|1132.0|\n",
      "|1900.0| 3.0|  2.0| 67750|  101.0|2650.0|\n",
      "| 715.0| 1.0|  1.0| 55177| 2591.0|1495.0|\n",
      "| 828.0| 2.0|  1.0| 45537| 8121.0|1265.0|\n",
      "|1183.0| 2.0|  2.0| 60513| 3792.0|1795.0|\n",
      "+------+----+-----+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the points from the rent database\n",
    "# I will apply a filter here to limit to a reasonable range \n",
    "# because I know the dataset has outliers (from a previous run)\n",
    "rent_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sqfeet,\n",
    "    beds, \n",
    "    baths,\n",
    "    income,\n",
    "    density,\n",
    "    price\n",
    "FROM rental_values\n",
    "WHERE \n",
    "    sqfeet <= 5000 AND\n",
    "    beds <= 8 AND\n",
    "    baths <= 8 AND\n",
    "    price BETWEEN 100 AND 4000\n",
    "\"\"\").repartition(1)\n",
    "rent_df.cache()\n",
    "rent_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------+-------+---------+\n",
      "|sqfeet|beds|baths|income|density|    price|\n",
      "+------+----+-----+------+-------+---------+\n",
      "|4000.0| 4.0|  4.0|106132| 1105.0|1200000.0|\n",
      "|3024.0| 3.0|  3.0|106132| 1105.0| 979900.0|\n",
      "|1400.0| 3.0|  3.0|106132| 1105.0| 539900.0|\n",
      "|2284.0| 4.0|  1.0|106132| 1105.0| 599900.0|\n",
      "|3697.0| 4.0|  4.0|106132| 1105.0| 929900.0|\n",
      "|4000.0| 4.0|  4.0|106132| 1105.0|1200000.0|\n",
      "|3024.0| 3.0|  3.0|106132| 1105.0| 979900.0|\n",
      "|1400.0| 3.0|  3.0|106132| 1105.0| 539900.0|\n",
      "|2284.0| 4.0|  1.0|106132| 1105.0| 599900.0|\n",
      "|3697.0| 4.0|  4.0|106132| 1105.0| 929900.0|\n",
      "+------+----+-----+------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    sqfeet,\n",
    "    beds, \n",
    "    baths,\n",
    "    income,\n",
    "    density,\n",
    "    price\n",
    "FROM sale_values\n",
    "WHERE \n",
    "    sqfeet <= 5000 AND\n",
    "    beds <= 8 AND\n",
    "    baths <= 8\n",
    "\"\"\").repartition(1)\n",
    "sale_df.cache()\n",
    "sale_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------------+\n",
      "|income|density|              label|\n",
      "+------+-------+-------------------+\n",
      "|111744|  895.0|       Cheshire, CT|\n",
      "| 42581| 4513.0|    Casselberry, FL|\n",
      "| 34283| 2849.0|New Port Richey, FL|\n",
      "| 40972| 2408.0|       Columbus, GA|\n",
      "| 51792|  197.0|      Middleton, ID|\n",
      "| 21737| 7703.0|    Springfield, MA|\n",
      "| 28929| 3237.0|        Pontiac, MI|\n",
      "| 51814|  493.0|    Summerville, SC|\n",
      "| 71628|  804.0|      Escondido, CA|\n",
      "| 44775|  326.0|         Tulare, CA|\n",
      "+------+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union together the location data \n",
    "# list partition block (state) first for faster compare\n",
    "loc_df = spark.sql(f\"\"\"\n",
    "(SELECT DISTINCT\n",
    "    income,\n",
    "    density,\n",
    "    label\n",
    "FROM rental_values)\n",
    "UNION\n",
    "(SELECT DISTINCT\n",
    "    income,\n",
    "    density,\n",
    "    label\n",
    "FROM sale_values)\n",
    "\"\"\").repartition(1)\n",
    "loc_df.cache()\n",
    "loc_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to parquet blocks\n",
    "# creates separate parts based on the partion value\n",
    "rent_df.write.mode(\"overwrite\").parquet(\"./outputs/rental\") \n",
    "sale_df.write.mode(\"overwrite\").parquet(\"./outputs/sales\") \n",
    "loc_df.write.mode(\"overwrite\").parquet(\"./outputs/location\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # kill spark when notebook is done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pyspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f6fa75b244e4a029fb3b8297a794ae4741a6a772f1757074dd55721f9a08d28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
